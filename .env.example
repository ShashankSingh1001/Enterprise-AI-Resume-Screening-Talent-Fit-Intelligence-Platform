# ========================================
# Enterprise AI Resume Screening Platform
# Environment Variables Template
# ========================================
# 
# INSTRUCTIONS:
# 1. Copy this file and rename it to .env
# 2. Fill in your actual values (especially KAGGLE credentials)
# 3. Never commit .env to Git (already in .gitignore)
#
# Last Updated: February 1, 2026
# Phase: 4 Ready (Modern NLP Pipeline)
# Deployment: Streamlit Community Cloud (FREE)
# ========================================

# ============ Application Settings ============
APP_NAME="Resume AI Platform"
APP_VERSION="1.0.0"
ENVIRONMENT="development"  # development, staging, production
DEBUG=True

# ============ API Settings ============
API_HOST="0.0.0.0"
API_PORT=8000
API_RELOAD=True  # Set to False in production

# ============ Streamlit Settings ============
STREAMLIT_HOST="0.0.0.0"
STREAMLIT_PORT=8501
STREAMLIT_THEME="light"

# ============ Database Configuration ============
# Using SQLite for free deployment (Streamlit Cloud compatible)
DATABASE_TYPE="sqlite"
DATABASE_PATH="data/database/resume_ai.db"

# PostgreSQL (Optional - Only for local testing if needed)
# DATABASE_HOST="localhost"
# DATABASE_PORT=5432
# DATABASE_NAME="resume_ai"
# DATABASE_USER="postgres"
# DATABASE_PASSWORD="your_password_here"
# DATABASE_URL="postgresql://${DATABASE_USER}:${DATABASE_PASSWORD}@${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}"

# Connection Pool Settings (for PostgreSQL only)
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10

# ============ Kaggle API Credentials (REQUIRED for Phase 3) ============
# Get from: https://www.kaggle.com/settings -> API -> Create New Token
# Download kaggle.json and copy username/key here
KAGGLE_USERNAME=""
KAGGLE_KEY=""

# ============ NLP Model Configuration (Phase 4 - Modern Techniques) ============
# Sentence Transformers for Semantic Matching
SENTENCE_TRANSFORMER_MODEL="all-MiniLM-L6-v2"
SENTENCE_TRANSFORMER_CACHE_DIR="models/nlp/sentence_transformers"
SENTENCE_TRANSFORMER_DEVICE="cpu"  # cpu or cuda

# KeyBERT for Keyword Extraction (2023 release, trending 2025)
KEYBERT_MODEL="all-MiniLM-L6-v2"
KEYBERT_TOP_N=15
KEYBERT_DIVERSITY=0.5
KEYBERT_CACHE_DIR="models/nlp/keybert"

# Zero-Shot Classification (HuggingFace BART)
ZEROSHOT_MODEL="facebook/bart-large-mnli"
ZEROSHOT_CACHE_DIR="models/nlp/zero_shot"
ZEROSHOT_DEVICE="cpu"

# Skill Extraction Settings
SKILL_TAXONOMY_PATH="src/nlp/skill_taxonomy.json"
SKILL_MATCH_THRESHOLD=0.75
SKILL_EXTRACTION_CONFIDENCE_THRESHOLD=0.6
SKILL_CATEGORIES="programming,frameworks,databases,devops,cloud,soft_skills,tools"

# Experience & Education Extraction
EXPERIENCE_SIMILARITY_THRESHOLD=0.7
EDUCATION_SIMILARITY_THRESHOLD=0.7

# ============ Model Settings ============
# Sentence-BERT (Legacy - keeping for backward compatibility)
SBERT_MODEL_NAME="all-MiniLM-L6-v2"
SBERT_CACHE_DIR="models/sbert_cache"

# spaCy - DEPRECATED for Phase 4 (using modern techniques instead)
# SPACY_MODEL="en_core_web_sm"

# XGBoost Classifier (Phase 7)
DEFAULT_MODEL_PATH="models/xgboost_model.pkl"
MODEL_THRESHOLD=0.5
MODEL_VERSION="v1"
RANDOM_STATE=42

# ============ MLflow Configuration (Phase 10) ============
# Using file-based tracking for free deployment
MLFLOW_TRACKING_URI="file:./mlruns"
MLFLOW_EXPERIMENT_NAME="resume_screening"
MLFLOW_ARTIFACT_LOCATION="./mlruns"
MLFLOW_REGISTRY_URI="sqlite:///mlruns.db"

# Model Registry
MLFLOW_MODEL_NAME="resume_fit_classifier"
MLFLOW_MODEL_STAGE="Production"  # Staging, Production, Archived
MLFLOW_ENABLE_SYSTEM_METRICS=True

# ============ DVC Configuration (Phase 10 - Optional) ============
# Using local storage by default (no cloud needed for portfolio)
DVC_REMOTE_NAME="local"
DVC_REMOTE_URL=""  # Leave empty for local storage
ENABLE_DVC_VERSIONING=False

# Cloud Storage Examples (if needed later):
# DVC_REMOTE_URL="s3://my-bucket/dvc-storage"
# DVC_REMOTE_URL="gs://my-bucket/dvc-storage"
# DVC_REMOTE_URL="azure://my-container/dvc-storage"

# ============ JWT Authentication ============
# Generate a secure key: python -c "import secrets; print(secrets.token_urlsafe(32))"
SECRET_KEY="your-secret-key-change-this-in-production"
ALGORITHM="HS256"
ACCESS_TOKEN_EXPIRE_MINUTES=30

# ============ File Upload Settings ============
MAX_UPLOAD_SIZE_MB=10
ALLOWED_RESUME_FORMATS="pdf,docx,doc,txt"
UPLOAD_DIR="data/uploads"

# ============ Feature Engineering (Phase 5) ============
MIN_EXPERIENCE_YEARS=0
MAX_EXPERIENCE_YEARS=50
SKILL_MATCH_WEIGHT=0.4
EXPERIENCE_WEIGHT=0.3
EDUCATION_WEIGHT=0.2
SIMILARITY_WEIGHT=0.1

# ============ Logging Configuration ============
LOG_LEVEL="INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE_PATH="logs/app.log"
LOG_MAX_BYTES=10485760  # 10MB
LOG_BACKUP_COUNT=5
LOG_FORMAT="%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# ============ CORS Settings ============
CORS_ORIGINS="http://localhost:3000,http://localhost:8501"
CORS_ALLOW_CREDENTIALS=True
CORS_ALLOW_METHODS="*"
CORS_ALLOW_HEADERS="*"

# ============ Explainability Settings (Phase 8) ============
SHAP_SAMPLE_SIZE=100
LIME_NUM_FEATURES=10
LIME_NUM_SAMPLES=1000
EXPLAINABILITY_OUTPUT_FORMAT="html"

# ============ Bias Audit Settings (Phase 9) ============
BIAS_SENSITIVE_FEATURES="gender,university_tier,years_of_experience"
FAIRNESS_THRESHOLD=0.8  # Disparate impact ratio threshold
FAIRNESS_METRICS="demographic_parity,equal_opportunity"

# ============ Cache Settings ============
ENABLE_CACHE=True
CACHE_TTL=3600  # seconds
ENABLE_MODEL_CACHING=True
MODEL_CACHE_DIR="models/cache"
CACHE_DIR=".cache"

# Redis (Optional - Not needed for portfolio project)
# REDIS_HOST="localhost"
# REDIS_PORT=6379
# REDIS_DB=0

# ============ Data Pipeline Paths (Phase 3-4) ============
# Raw Data
RAW_DATA_PATH="data/raw"
RAW_RESUME_DATASET="data/raw/resumes.csv"
RAW_JD_DATASET="data/raw/job_descriptions.csv"

# Processed Data
PROCESSED_DATA_PATH="data/processed"
PROCESSED_RESUME_PATH="data/processed/resumes_cleaned.csv"
PROCESSED_JD_PATH="data/processed/jds_cleaned.csv"
LABELED_DATA_PATH="data/processed/labeled_training_data.csv"

# Parsed Data (Phase 4 Output)
PARSED_RESUMES_PATH="data/processed/parsed_resumes.csv"
PARSED_JDS_PATH="data/processed/parsed_jds.csv"

# Features (Phase 5 Output)
FEATURES_DATA_PATH="data/features"
FEATURE_STORE_PATH="data/features/feature_store.csv"

# Interim/Analysis
INTERIM_DATA_PATH="data/interim"
SKILL_COVERAGE_REPORT="data/interim/skill_coverage_report.csv"
EXTRACTION_COMPARISON="data/interim/extraction_comparison.csv"

# Models & Logs
MODELS_PATH="models"
NLP_MODELS_PATH="models/nlp"
ML_MODELS_PATH="models/ml"
LOGS_PATH="logs"

# Dataset Limits (from Phase 3 decisions)
MAX_RESUMES=166
MAX_JDS=211
TRAINING_SAMPLES=2000
SELECTION_RATE=0.268

# ============ Performance Settings ============
# NLP Processing
BATCH_SIZE=32
NUM_WORKERS=4
MAX_RESUME_LENGTH=5000
MAX_JD_LENGTH=3000
PROCESSING_TIMEOUT=120

# API Performance
WORKER_THREADS=4
MAX_CONCURRENT_REQUESTS=100
REQUEST_TIMEOUT=300

# ============ Feature Flags ============
ENABLE_BIAS_AUDIT=True
ENABLE_EXPLAINABILITY=True
ENABLE_MLFLOW_TRACKING=True
ENABLE_USER_AUTH=True
ENABLE_EMAIL_NOTIFICATIONS=False

# ============ Testing Settings ============
TEST_DATABASE_URL="sqlite:///data/database/resume_ai_test.db"
TEST_MODE=False

# ============ Deployment Settings (Phase 14) ============
# Streamlit Cloud - These are auto-set by platform
# STREAMLIT_CLOUD_DEPLOYMENT=True
# RENDER_EXTERNAL_URL=""
# STREAMLIT_CLOUD_URL=""

# ========================================
# OPTIONAL/REMOVED - Not Needed for Portfolio
# ========================================
# These are removed to keep config clean for free deployment:
#
# OpenAI (not using LLMs for Phase 4)
# OPENAI_API_KEY=""
#
# AWS (not using S3 for portfolio)
# AWS_ACCESS_KEY_ID=""
# AWS_SECRET_ACCESS_KEY=""
# AWS_REGION="us-east-1"
# AWS_S3_BUCKET=""
#
# Google Cloud (not using GCS for portfolio)
# GOOGLE_APPLICATION_CREDENTIALS=""
# GCS_BUCKET_NAME=""
#
# Monitoring (not needed for portfolio)
# SENTRY_DSN=""
# GA_TRACKING_ID=""
#
# Email Notifications (not needed for portfolio)
# SMTP_HOST="smtp.gmail.com"
# SMTP_PORT=587
# SMTP_USER=""
# SMTP_PASSWORD=""
# SMTP_FROM_EMAIL=""

# ========================================
# END OF CONFIGURATION
# ========================================
# 
# DEPLOYMENT NOTES:
# - For Streamlit Cloud: Add sensitive vars to Secrets management
# - For local dev: Copy to .env and fill in actual values
# - Total cost: $0 (100% free deployment stack)
# ========================================